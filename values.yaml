# Default values for spark chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# meta:
#   owner:
#   gitrepo:

replicaCount: 1

project:
  # default service/component name, free to change
  component: gdt
  namespace: default

applicationType: workflow

appLabels:
  # custom:
  #   key: value
  custom: {}

runAsJob: false

image:
  repository: rehanann/spark-py
  tag: 3.4.0-extended
  pullPolicy: Always
  command: ["/opt/java/openjdk/zulu11.66.15-ca-jdk11.0.20-linux_x64/bin/java"]
  args:
    - "-cp"
    - "/opt/spark/conf/:/opt/spark/jars/*"
    - "-Dspark.history.fs.logDirectory=s3a://historyservice/logs"
    - "-Dspark.hadoop.fs.s3a.endpoint=http://minio.default.svc.cluster.local:9000"
    - "-Dspark.hadoop.fs.s3a.access.key=bU0c4naTz98KynxEsHf1"
    - "-Dspark.hadoop.fs.s3a.secret.key=dNOkRi7cbUsw0ZHnN9coosKO0iyu21XJpieAayZd"
    - "-Dspark.hadoop.fs.s3a.path.style.access=true"
    - "-Dspark.hadoop.fs.s3a.connection.ssl.enabled=false"
    - "-Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
    - "-Dspark.history.fs.cleaner.enabled=true"
    - "-Dspark.history.fs.cleaner.maxAge=7d"
    - "-Xmx1g"
    - "org.apache.spark.deploy.history.HistoryServer"
#  command: ["/opt/spark/bin/spark-submit"]
# environment variables
envVars:
  driver:
    SPARK_LOG_DIR: /opt/spark/work-dir
  executor:
    SPARK_LOG_DIR: /opt/spark/work-dir

# This allow to add any required key values to spark configuration, unless it should not duplicated.
# e.g.
# sparkconf:
#  spark.sql.maxPlanStringLength: "2147483632"
sparkConf:
  spark.hadoop.fs.s3a.endpoint: http://minio.default.svc.cluster.local:9000
  spark.hadoop.fs.s3a.access.key: bU0c4naTz98KynxEsHf1
  spark.hadoop.fs.s3a.secret.key: dNOkRi7cbUsw0ZHnN9coosKO0iyu21XJpieAayZd
  spark.hadoop.fs.s3a.path.style.access: true
  spark.hadoop.fs.s3a.connection.ssl.enabled: false
  spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3native.NativeS3FileSystem
  spark.history.fs.cleaner.enabled: true
  spark.history.fs.cleaner.maxAge: 7d

pyspark:
  python: /usr/bin/python3
  requirements:
    #Fixed configmap file path location to /tmp/requirements.txt, this removes this PVC mandatory requirements.
    filePath: /tmp/pip3_requirements.txt

sparkClientPod:
  required: true

services:
  ui:
    enabled: true
    type: ClusterIP
    port: 18080
  driver:
    enabled: true
    type: ClusterIP
    port: 3000

driver:
  cores: 1
  memory: 1g
  limit:
    cores: 1
  request:
    cores: 1
executor:
  cores: 1
  memory: 1g
  memoryOverhead: 1g
  request:
    cores: 1
  limit:
    cores: 1

resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 1
    memory: 1Gi

dynamicAllocation:
  enabled: false
  executorIdleTimeout: 30s
  # Values are chosen based on current YARN setting and existing quotas
  executors:
    minExecutors: 1
    initialExecutors: 1
    maxExecutors: 2
  shuffle:
    # disable external shuffle service, in k8s it is not used
    service:
      enabled: false
    # instead of external shuffle service, tracking of pods which have shuffle blocks is used
    tracking:
      enabled: false
    # timeout for the the pod to keep shuffle blocks
    timeout: 30s

sparkLogs: ERROR

sharedVolume:
  enabled: false
  useExisting: true
  mountPath: /opt/spark/work-dir/shared
  existingPvcName: gdt-shared

eventLogs:
  # spark.eventLog.enabled
  enabled: true
  # spark.eventLog.dir
  #  Example for shared PVC
  #  directory: /opt/spark/work-dir/shared/history
  # directory: /tmp/spark-events
  directory: s3a://historyservice/logs
  persistent: false
  #

historyServer:
  enabled: true
